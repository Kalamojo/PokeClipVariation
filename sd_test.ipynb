{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3523443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Howdy\n"
     ]
    }
   ],
   "source": [
    "print(\"Howdy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7148ef7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217133fee8064bce839ba33149005dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd26b25c7ced4d7c8c97577157b6d757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from diffusers.utils import load_image\n",
    "#from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "\n",
    "# switch to \"mps\" for apple devices\n",
    "#pipe = DiffusionPipeline.from_pretrained(\"lambdalabs/sd-image-variations-diffusers\", dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "#pipe = StableDiffusionImageVariationPipeline.from_pretrained(\"/home/kojo/Code/sd-image-variations-diffusers/\", local_files_only=True)\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_single_file(\"/home/kojo/Code/stable-diffusion-v1-5/v1-5-pruned-emaonly.safetensors\")\n",
    "pipe.feature_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "994bf6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableDiffusionImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.36.0\",\n",
       "  \"feature_extractor\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c049948",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.load_ip_adapter(\"/home/kojo/Code/IP-Adapter/\", subfolder=\"models\", weight_name=\"ip-adapter-plus_sd15.safetensors\", image_encoder_folder=\"image_encoder\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4da6cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StableDiffusionImg2ImgPipeline {\n",
       "  \"_class_name\": \"StableDiffusionImg2ImgPipeline\",\n",
       "  \"_diffusers_version\": \"0.36.0\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPVisionModelWithProjection\"\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02c64cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"/home/kojo/Code/IP-Adapter/\", \n",
    "    subfolder=\"models/image_encoder\", \n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "feature_extractor = CLIPImageProcessor.from_pretrained(\n",
    "    \"/home/kojo/Code/IP-Adapter/\", \n",
    "    subfolder=\"models/image_encoder\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39664a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABgAGADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACinFGEYkx8pJUH3GP8abQAUUUUAFFFFABRRRQAUUUUAFKqlmCjGScDJxSVasrOa7eRokLCFDI59AKBpNuyO003RrKT7Ha3sPmpaIJZI0kI3u55BPb7nT37Vq6hb6X9nK2ulwLA5CxW6nduJOfvHn3J9ATWPA7w291HGhadyN20Z/d7AQPqSx/P6VJaWt22oGNpcSRqqTSKei8HYvcZ/DHXr1lrzPoaVdRUYRp6y0fl5P0S1MDV9KnitYxZxM1mhJBHJkPQvgc7eQBknj61hwQS3MnlwoWbGcV6rKojhQxpgREEBV6AcHAHtniub1HTo7Oe4vbZkC3C5V8DamQcHPTGev/AAHrmiLueRmEY4fGRpVNIyWj89rf5ficVRWtpUcdvFLqFxFvEanygf7/AGJHcZ9/WspmLsWYksTkk96o5nFJJ31EooooJCiiigArp/CV6tut1EsKtMw3K/sATz7cfmRXMVb06/l06686MKwZSjo4yrqeCCPp+I6jBwaUttDbD1XRqKZ6HFZxw2bp5shQMPuHBbaAuPrxgY9Ac55qDym0qaO4ZI/JYeWyxKfk9MknnkYzxk9ua5+bxK0drHbbvNKYPmIB8xBBBJPfv9euaS58Vfat8UNsbaK58rz0WQv8yZwVLZIBJBwSTwOccVD5k7JX/r+v+AezUx+GTTitrW/r8Ga1zqdu7TTCVmmEojgRcgjBGTzUoliudNeKRQImSRU3AEKjNkHHbau0gfQcVy9xdwQ3hOUc7SWaIdT6c5H/AOuq1zqstwm0gncuG3nPPqB0Hf8AM1o12PAzJfWmlF7O9+vX/PX0S6E2o3+LOGwgdPJjGTsUAk+7A8n1xxWTRRQXObm7sKKKKCAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAAAABnRSTlMAAAAAAABupgeRAAAFJklEQVR4Ae2ZIXQbRxCGL30FgicYFkHBC2tZHGYYM5c5tNCsKWrCBENtlJYlUNBhLrNYjvXCAnWsgv3WvzxvvTpbinJ7SfPmnt5pdnZvd+fTv7N7dlH45QScgBNwAk7ACTgBJ+AEnIATcAJOwAk4ASfgBJyAE3ACTsAJOAEn4AScgBNwAk7ACTgBJ+AEiuLkUfV/xPDDkJMuy3LI4XoZ68deetnaCfL5ZTx51zZJy9fFE/P8Wrw3G0OKO/+4iJ3D2wMBSgIzLtNqbFUny7AGRURAj5oLq/1axhCAFK0iFJpnJz8/fDzF8+mqrhe1qpCYDIT2+GN51F60bSvPV7wPASgOD8lMq+kmHWsDnbeTg1BMl6M1GdQYDhDrRZEjGVNNEqu1wfgW5MP0su9i8fpKcCRFoLxsQxqqF0vuPDg6/ClpM3wxLyCCJJvsHpVy9otygdZISccfVvcwonM+u3e+X8u8gEgoxDwvlqfFp2R+iSdeXOscdP3A6nACo01Mn4s+GX33YvYclIDQzMIiuvntQYMzhmKzD/varD4/DfvdqChW87+t6n5DyurlDJVXQXeFcVatVAWdyWTKB5XhMR1tPigpyX+/fKjtPJRu9rmLZyBATVObRkxToqNZXhSrTjqEejJbH5RoGb+sXD1qNzUiOn8um742wYxLLA7GfitbXCBDOObfxUBEgdFlvRiPFuNVFTR36xIdXJvvNLfafU5hIAXZlGxxmUdGLLGkKim2Lw9pnDhVhIvJs7PBHs5cgPgxbTYHIcOGSwccDOEw/+50QiaaN2KEjuJRsCVJlmpf64up5gJE10o6zxejw2JMkd9W8pkVDylyyb+VDmmofDHXI8PfswCyXEA8pFKLCljQIYNawobOLpmI9sYoFpElaUZEUDZQj0YWQDY/ArN8CZr4jxssN9vFbOnZgxg6XqI7nQDiqk7bhNlZu7ezf0CxfDanhXxYMqDR+4QabGZuoLDxqxaDYiwi+clE2ihtRBghyR4TEAP1D0iztzsrSAcck4mKWmV3/eyiQxZXA4Nl3W4aoKfnfukwSsZzEL2zQKrlajY5gI7WF/Lho/CIRwYQrQEeW1PK4mpz153XtHeXtRZy73QYtH9ASpxQ4CctxiPDkUQoHcmJfVZNZzctTD43jui7aUZ1oROj9nvu+iNJOFi3i6hpP2b/gDSvdfyBUXDE6ZlivJFRZLnRvm7DNmQtJR/URFrBH2pLvq+vpjFjdElt1fnasW7zZV+5AMWzipdP7O+0adyUN0SK4qwMokAjJPX2+TTI5/r9xIxn5eSq7Hgp6+x8D2cWQGQEyb5zQtrIkiqJKHkKKEkzFY1OeVafZ/7Dfq5d7N/j9wRjh6A4ztiJXkKqOp6Hz+3L6CTyER3Q8MmRlW/PIs82z7xfzcvqjzn6J7lYWonHFhpOQDEvNYh1lNABCkmHOxvfLkfweMT97Af7Pbb1KR3h3v62Ono1IkgxAorePIDyz5twDnz6exXexa6bqc+YjonIhhMUvaD0fia0UWIjFyDGiP8epLChY29M4kKz078CIwwLPp6fyURQqBIXOh9gfTFcRkD0ThhESEisNf69gSJUpIrwqMXQpaLhwGkg9Ii1x1g/M8jXEIAULfetsQmoAjdAuzyoR3Lcc+1imitEiFMRbqUThyc6ezwYd/Id2iiI6zsMzENyAk7ACTgBJ+AEnIATcAJOwAk4ASfgBJyAE3ACTsAJOAEn4AScgBNwAk7ACTgBJ+AEnIATcAJO4Fsk8B8l23pzgDQPAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=96x96>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(image):\n",
    "    with torch.no_grad():\n",
    "        image_features = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        #image_features = pipe.feature_extractor(images=image, return_tensors=\"pt\").to(\"cpu\")\n",
    "        embedding = image_encoder(image_features).image_embeds\n",
    "        #outputs = image_encoder(**image_features, output_hidden_states=True)\n",
    "        #embedding = outputs.hidden_states[-2]\n",
    "        \n",
    "        #embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "    return embedding\n",
    "\n",
    "starting_image = load_image(\"./venipede.png\")\n",
    "starting_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a52a02d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_embed = get_embedding(starting_image)\n",
    "starting_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "23c37a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4115,  0.3435,  1.1032,  ..., -0.6856,  0.2965, -0.2154]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a655c6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_embeds = torch.unsqueeze(starting_embed, 0)\n",
    "starting_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30b6f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=96x96>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt = \"Turn this cat into a dog\"\n",
    "input_image = load_image(\"./scolipede.png\")\n",
    "\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "463d9572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_embed = torch.zeros_like(starting_embeds)\n",
    "negative_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10dae43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1024])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_embeds = torch.cat([negative_embed, starting_embeds], dim=0)\n",
    "combined_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16e28637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a942f7ba5b43fca11a38c6d0728b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x1024 and 1280x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m image = \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip_adapter_image_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombined_embeds\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7.5\u001b[39;49m\u001b[43m)\u001b[49m.images[\u001b[32m0\u001b[39m]\n\u001b[32m      3\u001b[39m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:1101\u001b[39m, in \u001b[36mStableDiffusionImg2ImgPipeline.__call__\u001b[39m\u001b[34m(self, prompt, image, strength, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[39m\n\u001b[32m   1098\u001b[39m latent_model_input = \u001b[38;5;28mself\u001b[39m.scheduler.scale_model_input(latent_model_input, t)\n\u001b[32m   1100\u001b[39m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1111\u001b[39m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_classifier_free_guidance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/diffusers/models/unets/unet_2d_condition.py:1101\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.time_embed_act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1099\u001b[39m     emb = \u001b[38;5;28mself\u001b[39m.time_embed_act(emb)\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m encoder_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_encoder_hidden_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[38;5;66;03m# 2. pre-process\u001b[39;00m\n\u001b[32m   1106\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.conv_in(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/diffusers/models/unets/unet_2d_condition.py:973\u001b[39m, in \u001b[36mUNet2DConditionModel.process_encoder_hidden_states\u001b[39m\u001b[34m(self, encoder_hidden_states, added_cond_kwargs)\u001b[39m\n\u001b[32m    970\u001b[39m         encoder_hidden_states = \u001b[38;5;28mself\u001b[39m.text_encoder_hid_proj(encoder_hidden_states)\n\u001b[32m    972\u001b[39m     image_embeds = added_cond_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_hid_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     encoder_hidden_states = (encoder_hidden_states, image_embeds)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/diffusers/models/embeddings.py:2606\u001b[39m, in \u001b[36mMultiIPAdapterImageProjection.forward\u001b[39m\u001b[34m(self, image_embeds)\u001b[39m\n\u001b[32m   2604\u001b[39m batch_size, num_images = image_embed.shape[\u001b[32m0\u001b[39m], image_embed.shape[\u001b[32m1\u001b[39m]\n\u001b[32m   2605\u001b[39m image_embed = image_embed.reshape((batch_size * num_images,) + image_embed.shape[\u001b[32m2\u001b[39m:])\n\u001b[32m-> \u001b[39m\u001b[32m2606\u001b[39m image_embed = \u001b[43mimage_projection_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m image_embed = image_embed.reshape((batch_size, num_images) + image_embed.shape[\u001b[32m1\u001b[39m:])\n\u001b[32m   2609\u001b[39m projected_image_embeds.append(image_embed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/diffusers/models/embeddings.py:2303\u001b[39m, in \u001b[36mIPAdapterPlusImageProjection.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   2294\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[32m   2295\u001b[39m \n\u001b[32m   2296\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2299\u001b[39m \u001b[33;03m    torch.Tensor: Output Tensor.\u001b[39;00m\n\u001b[32m   2300\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2301\u001b[39m latents = \u001b[38;5;28mself\u001b[39m.latents.repeat(x.size(\u001b[32m0\u001b[39m), \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2303\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproj_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m   2306\u001b[39m     residual = latents\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pokeclip/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (2x1024 and 1280x768)"
     ]
    }
   ],
   "source": [
    "image = pipe(prompt=\"\", image=input_image, ip_adapter_image_embeds=[combined_embeds], strength=0.99, guidance_scale=7.5).images[0]\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pokeclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
